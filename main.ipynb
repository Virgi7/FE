{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import scipy as sc\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import utilities as ut\n",
    "import utilities_2 as ut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv('EUROSTOXX50_2023_Dataset.csv', index_col=0) #we read the csv file and convert it in a dataframe on python \n",
    "\n",
    "name_stocks0=['ADSGn.DE', 'ALVG.DE', 'MUVGn.DE',\n",
    "                            'OREP.PA'] #we select the required stocks for the Exercise 0\n",
    "dates_num=['2016-03-18','2019-03-20'] #we choose a 3y estimation starting from today (20th March 2019) \n",
    "                                           #and going backward up to the first business day before 21st March 2016\n",
    "dates_den = ['2016-03-17','2019-03-19']\n",
    "\n",
    "np_num, np_den=ut.read_our_CSV(df,name_stocks0, dates_num, dates_den) #we call our read csv function to convert the dataframe in numpy arrays\n",
    "                                                                        #paying attention to the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Parameters\n",
    "alpha=0.95\n",
    "notional=1e7\n",
    "delta=1\n",
    "n_asset=4\n",
    "weights=np.ones((n_asset,1))/n_asset #we consider a equally weighted portfolio\n",
    "returns=np.log(np_num/np_den) #computation of the returns\n",
    "\n",
    "VaR, ES = ut.AnalyticalNormalMeasures(alpha,weights,notional,delta,returns) #we compute the VaR and the ES using a Gaussian parametric approach\n",
    "print(\"VaR_0:\", VaR, \"ES_0:\", ES)\n",
    "\n",
    "VaR_check=ut.plausibilityCheck(returns, weights, alpha, notional, delta) #we check the result with a Plausibility check to estimate the order of magnitude of portfolio VaR\n",
    "print(\"VaR check_0:\", VaR_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1a\n",
    "#Parameters\n",
    "sett_date1='2019-03-20'\n",
    "alpha_1=0.99\n",
    "dates_num1=dates_num\n",
    "dates_den1 = dates_den #3y estimation as before\n",
    "shares1=np.array([25000, 20000, 20000, 10000])\n",
    "Nsim=200\n",
    "name_stocks1a=['TTEF.PA', 'DANO.PA', 'SASY.PA', 'VOWG_p.DE'] #we select the required stocks for the Exercise 1a)\n",
    "\n",
    "stockPrice_1a=df.loc[[sett_date1],name_stocks1a].to_numpy() #we extract the price of the stocks at the sett_date \n",
    "                                                                #and then we convert the chosen row in a numpy array to perform calculation\n",
    "ptf_value1a=shares1.dot(stockPrice_1a.T) #value of the portfolio\n",
    "weights_1a=(shares1*stockPrice_1a/ptf_value1a).T #we compute the corresponding weight of each stock related with its number of shares\n",
    "np_num1a, np_den1a=ut.read_our_CSV(df,name_stocks1a, dates_num1, dates_den1) \n",
    "logReturns_1a=np.log(np_num1a/np_den1a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ES_HSM, VaR_HSM= ut2.HSMeasurements(logReturns_1a, alpha_1, weights_1a, ptf_value1a, delta) #we compute the VaR and the ES via a Historical Simulation\n",
    "print(\"VaR_HSM:\", VaR_HSM,\"ES_HSM:\", ES_HSM)\n",
    "\n",
    "#CHECK\n",
    "VaR_check_HSM=ut.plausibilityCheck(logReturns_1a, weights_1a, alpha_1, ptf_value1a, delta) #as we did previously we check the result\n",
    "print(\"VaR_check_HSM:\", VaR_check_HSM)\n",
    "\n",
    "samples_Bootstrap=ut2.bootstrapStatistical(Nsim, logReturns_1a) #we call the Bootstrap function to extract randomly Nsim partial sets of risk factors \n",
    "VaR_boot = ut2.HSMeasurements(samples_Bootstrap, alpha_1, weights_1a, ptf_value1a, delta) #then we pass the samples of risk factors to the HS function to compute the VaR one for each simulation\n",
    "print(\"VaR_Bootstrap:\", np.mean(VaR_boot)) #as output we print the mean of the computed VaRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1b\n",
    "#Parameters\n",
    "Nsim=200\n",
    "name_stocks1b=['ADSGn.DE', 'AIR.PA', 'BBVA.MC', 'BMWG.DE', 'SCHN.PA'] #we select the required stocks for the Exercise 1b)\n",
    "Lambda=0.97\n",
    "n_asset1b=len(name_stocks1b)\n",
    "weights_1b=np.ones((n_asset1b,1))/n_asset1b #we have to consider a equally weighted ptf\n",
    "stockPrice_1b=df.loc[[sett_date1],name_stocks1b].to_numpy() #as before we extract the row corresponding to the value of the stocks on the sett_date\n",
    "ptf_value1b=1e7 #we set the ptf value equal to the notional 10Mln\n",
    "\n",
    "np_num1b, np_den1b=ut.read_our_CSV(df,name_stocks1b, dates_num1, dates_den1)\n",
    "logReturns_1b=np.log(np_num1b/np_den1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ES_WHS, VaR_WHS=ut2.WHSMeasurements(logReturns_1b, alpha_1, Lambda, weights_1b, ptf_value1b, delta) #we compute the VaR and the ES via a Weighted Historical Simulation\n",
    "print(\"VaR_WHS:\", VaR_WHS, \"ES_WHS:\",ES_WHS)\n",
    "#CHECK\n",
    "VaR_check_WHS = ut.plausibilityCheck(logReturns_1b, weights_1b, alpha_1, ptf_value1b, delta) #check of the result\n",
    "print(\"VaR_check_WHS:\", VaR_check_WHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 1c\n",
    "#Parameters\n",
    "N=20\n",
    "df_1c=df.loc[:, df.columns != 'ADYEN.AS'] #we remove the column corresponding to the Adyen stock due to missing data\n",
    "df_1c=df_1c.iloc[: , :N] #we select the first 20 stocks of the new dataset\n",
    "name_stocks1c=df_1c.columns\n",
    "np_num1c, np_den1c=ut.read_our_CSV(df,name_stocks1c, dates_num1, dates_den1)\n",
    "logReturns_1c=np.log(np_num1c/np_den1c)\n",
    "\n",
    "n_asset1c=len(name_stocks1c)\n",
    "weights_1c=np.ones((n_asset1c,1))/n_asset1c #we have to consider as above a equally weighted ptf\n",
    "ptf_value1c=1e8\n",
    "days_VaR1c=10 #now the VaR must be computed at 10 days\n",
    "n=range(1,7) #parameter used for the PCA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialization\n",
    "ES_PCA=np.zeros((len(n),1))\n",
    "VaR_PCA=np.zeros((len(n),1))\n",
    "yearlyCovariance= np.cov(logReturns_1c.T) #we compute the Variance Covariance Matrix of the returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.59571040e-04],\n",
       "       [ 1.91554663e-04],\n",
       "       [ 9.60370953e-04],\n",
       "       [ 8.87035185e-04],\n",
       "       [ 3.48883592e-04],\n",
       "       [ 4.03908972e-04],\n",
       "       [ 8.51264567e-04],\n",
       "       [ 6.63886038e-05],\n",
       "       [ 4.90134130e-05],\n",
       "       [-5.64596656e-04],\n",
       "       [-1.40744798e-04],\n",
       "       [-1.53646409e-04],\n",
       "       [-2.33009399e-05],\n",
       "       [ 1.49235758e-04],\n",
       "       [ 1.21935162e-04],\n",
       "       [ 5.57627395e-04],\n",
       "       [ 2.64072633e-04],\n",
       "       [-1.07331297e-05],\n",
       "       [ 4.46853034e-04],\n",
       "       [ 1.91798677e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearlyMeanReturns= np.mean(logReturns_1c, axis=0).reshape((20,1)) #we compute the mean of each column (i.e. columns <-> returns of the stocks) of the matrix of the returns,\n",
    "                                                                    #then we reshape it to have a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in n:\n",
    "    ES_PCA[i-1], VaR_PCA[i-1]= ut2.PrincCompAnalysis(yearlyCovariance, yearlyMeanReturns, weights_1c, days_VaR1c, alpha_1, i, \n",
    "                      ptf_value1c) #for each i in the set of n we compute the PCA increasing at each iteration the number of principal components to be considered\n",
    "print(\"VaR_PCA:\", VaR_PCA, \"ES_PCA:\", ES_PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK\n",
    "VaR_check_PCA = ut.plausibilityCheck(logReturns_1c, weights_1c, alpha_1, ptf_value1c, days_VaR1c) #check of the result\n",
    "print(\"VaR_check_PCA:\", VaR_check_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 2\n",
    "#Parameters\n",
    "sett_date2='2023-01-31'\n",
    "expiry2='2023-04-05' #expiry of the puts\n",
    "strike=25\n",
    "value_ptf2=25870000\n",
    "volatility=0.154\n",
    "dividend=0.031\n",
    "alpha_2=0.99\n",
    "days_VaR=10\n",
    "rate=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We select only the stocks of Vonovia between the settlement date and 2y before\n",
    "name_stocks2=['VNAn.DE']\n",
    "n_asset2=len(name_stocks2)\n",
    "weights_2=np.ones((n_asset2,1))/n_asset2 #equally weighted ptf\n",
    "dates_num2=['2021-02-01',sett_date2]\n",
    "dates_den2 = ['2021-01-29','2023-01-30'] #2y estimation using the Historical Simulation for the underlying\n",
    "np_num2, np_den2=ut.read_our_CSV(df,name_stocks2, dates_num2, dates_den2)\n",
    "\n",
    "stockPrice_2=np_num2[len(np_num2)-1] #as before we select the stock price at the sett_date\n",
    "numberOfShares=value_ptf2/stockPrice_2 #number of the shares as the product between the weight of the stock (in this case equal to 1, since ptf composed by only one stock) \n",
    "                                            #and the value of the total ptf divided by the stock price at the sett_date\n",
    "numberOfPuts=numberOfShares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert the string format of the dates to get the difference in days between the sett_date and the expiry\n",
    "start = datetime.strptime(sett_date2, \"%Y-%m-%d\")\n",
    "end = datetime.strptime(expiry2, \"%Y-%m-%d\")\n",
    "diff = end - start\n",
    "timeToMaturityInYears=diff.days/365 #ttm in days\n",
    "riskMeasureTimeIntervalInYears=days_VaR/365\n",
    "NumberOfDaysPerYears=np.busday_count('2022-01-01', '2023-01-01') #we compute the number of business days in a year\n",
    "logReturns_2=np.log(np_num2/np_den2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VaR_MC=ut2.FullMonteCarloVaR(logReturns_2, numberOfShares, numberOfPuts, stockPrice_2, strike, rate, dividend,\n",
    "                              volatility, timeToMaturityInYears, riskMeasureTimeIntervalInYears, alpha_2, NumberOfDaysPerYears) #we compute the VaR at 10 days via a Full MonteaCarlo approach            \n",
    "VaR_DN=ut2.DeltaNormalVaR(logReturns_2, numberOfShares, numberOfPuts, stockPrice_2, strike, rate, dividend,                            \n",
    "                            volatility, timeToMaturityInYears, riskMeasureTimeIntervalInYears, alpha_2, NumberOfDaysPerYears) #we compute the VaR at 10 days via a Delta Normal approach\n",
    "print(\"VaR_MC:\", VaR_MC, \"VaR_DN:\", VaR_DN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
